import os
import datetime
import requests
from typing import List
from pydantic import BaseModel, Field
from langchain.tools import tool
from langchain_tavily import TavilySearch
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import json
from urllib.parse import quote

class ToolManager:
    def __init__(self, env_path: str = "è¯¾ç¨‹åŠ©æ‰‹/lna.env"):
        """
        åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨
        :param env_path: .env æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv(env_path)

        # åˆå§‹åŒ– Tavily æœç´¢å·¥å…·
        self.search_tool = TavilySearch(max_results=5, topic="general")

    # ==================== å·¥å…·å®šä¹‰ ====================

    # âœ… ç½‘é¡µæŠ“å–å·¥å…· Schema å’Œå‡½æ•°
    class WebScrapingSchema(BaseModel):
        url: str = Field(description="è¦æŠ“å–çš„ç½‘é¡µURL")
        extract_text: bool = Field(default=True, description="æ˜¯å¦åªæå–æ–‡æœ¬å†…å®¹")

    @tool(args_schema=WebScrapingSchema)
    def web_scraping(url: str, extract_text: bool = True) -> str:
        """
        æŠ“å–æŒ‡å®šç½‘é¡µçš„å†…å®¹ã€‚å¯ä»¥è·å–ç½‘é¡µçš„æ–‡æœ¬å†…å®¹æˆ–HTMLæºç ã€‚
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding

            if extract_text:
                soup = BeautifulSoup(response.text, 'html.parser')
                for script in soup(["script", "style"]):
                    script.decompose()
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = '\n'.join(chunk for chunk in chunks if chunk)
                return text[:3000] + "\n\n[å†…å®¹å·²æˆªæ–­...]" if len(text) > 3000 else text
            else:
                return response.text[:3000] + "\n\n[HTMLå†…å®¹å·²æˆªæ–­...]" if len(response.text) > 3000 else response.text
        except Exception as e:
            return f"ç½‘é¡µæŠ“å–å¤±è´¥: {str(e)}"

    # âœ… æ—¥æœŸæ—¶é—´å·¥å…· Schema å’Œå‡½æ•°
    class DateTimeSchema(BaseModel):
        operation: str = Field(description="æ“ä½œç±»å‹: now, format, calculate, timezone")
        date_string: str = Field(default="", description="æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç”¨äºæ ¼å¼åŒ–æˆ–è®¡ç®—ï¼‰")
        format_string: str = Field(default="%Y-%m-%d %H:%M:%S", description="æ—¥æœŸæ ¼å¼")
        days_offset: int = Field(default=0, description="å¤©æ•°åç§»é‡ï¼ˆç”¨äºæ—¥æœŸè®¡ç®—ï¼‰")

    @tool(args_schema=DateTimeSchema)
    def datetime_operations(operation: str, date_string: str = "", format_string: str = "%Y-%m-%d %H:%M:%S", days_offset: int = 0) -> str:
        """
        æ‰§è¡Œæ—¥æœŸæ—¶é—´ç›¸å…³æ“ä½œï¼ŒåŒ…æ‹¬è·å–å½“å‰æ—¶é—´ã€æ ¼å¼åŒ–æ—¥æœŸã€æ—¥æœŸè®¡ç®—ç­‰ã€‚
        """
        try:
            if operation == "now":
                return datetime.datetime.now().strftime(format_string)
            elif operation == "format":
                if date_string:
                    dt = datetime.datetime.fromisoformat(date_string.replace('Z', '+00:00'))
                    return dt.strftime(format_string)
                else:
                    return "è¯·æä¾›è¦æ ¼å¼åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸²"
            elif operation == "calculate":
                base_date = datetime.datetime.now()
                if date_string:
                    base_date = datetime.datetime.fromisoformat(date_string.replace('Z', '+00:00'))
                new_date = base_date + datetime.timedelta(days=days_offset)
                return new_date.strftime(format_string)
            elif operation == "timezone":
                import time
                utc_offset = time.timezone // 3600
                dst_offset = time.altzone // 3600 if time.daylight else utc_offset
                return f"å½“å‰æ—¶åŒº: {time.tzname[0]}/{time.tzname[1] if time.daylight else ''}, UTCåç§»: {utc_offset if not time.daylight else dst_offset} å°æ—¶"
            else:
                return f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}"
        except Exception as e:
            return f"æ—¥æœŸæ—¶é—´æ“ä½œå¤±è´¥: {str(e)}"

    # âœ… å¤©æ°”æŸ¥è¯¢å·¥å…· Schema å’Œå‡½æ•°
    class WeatherQuerySchema(BaseModel):
        city: str = Field(..., description="è¦æŸ¥è¯¢å¤©æ°”çš„åŸå¸‚åç§°ï¼Œå¦‚ åŒ—äº¬ã€ä¸Šæµ·ã€çº½çº¦")

    @tool(args_schema=WeatherQuerySchema)
    def get_realtime_weather(city: str) -> str:
        """
        æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å®æ—¶å¤©æ°”æƒ…å†µï¼Œä½¿ç”¨ wttr.in å…è´¹æœåŠ¡ã€‚
        - æ”¯æŒä¸­æ–‡åŸå¸‚å
        - æ— éœ€ API Key
        è¿”å›ï¼šå¤©æ°”çŠ¶å†µã€æ¸©åº¦ã€é£é€Ÿã€æ¹¿åº¦ç­‰ä¿¡æ¯ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰
        """
        url = f"https://wttr.in/{city}"
        params = {
            "format": 2,  # ç®€æ´æ ¼å¼ï¼šåŸå¸‚: å¤©æ°”, æ¸©åº¦
            "lang": "zh"  # ä¸­æ–‡æ˜¾ç¤º
        }
        try:
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                return response.text.strip()
            else:
                return f"å¤©æ°”æŸ¥è¯¢å¤±è´¥ï¼šHTTP {response.status_code}"
        except Exception as e:
            return f"æŸ¥è¯¢å¤©æ°”æ—¶å‡ºé”™: {str(e)}"




    # âœ… åˆ›å»ºç™¾åº¦æœç´¢å·¥å…·
    class BaiduSearchSchema(BaseModel):
        query: str = Field(description="æœç´¢æŸ¥è¯¢å…³é”®è¯")
        num_results: int = Field(default=5, description="è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤5ä¸ª")
    
    @tool(args_schema=BaiduSearchSchema)
    def baidu_search(query: str, num_results: int = 5) -> str:
        """
        ä½¿ç”¨ç™¾åº¦æœç´¢è·å–ä¿¡æ¯ï¼Œå¯¹ä¸­æ–‡æœç´¢å‹å¥½ã€‚
        ç™¾åº¦ç›¸æ¯”è°·æ­Œå¯¹çˆ¬è™«æ›´å®½æ¾ï¼Œé€‚åˆä¸­æ–‡å†…å®¹æœç´¢ã€‚
        """
        
        # æ„å»ºç™¾åº¦æœç´¢URL
        search_url = "https://www.baidu.com/s"
        params = {
            'wd': query,
            'pn': 0,  # é¡µç ï¼Œç™¾åº¦æ˜¯10ä¸ªç»“æœä¸€é¡µ
            'rn': num_results if num_results <= 50 else 50,  # ç™¾åº¦æœ€å¤šè¿”å›50ä¸ªç»“æœ
            'ie': 'utf-8'
        }
        
        # ç™¾åº¦æœç´¢è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none'
        }
        
        try:
            # å…ˆå°è¯•ä¸ä½¿ç”¨ä»£ç†
            try:
                response = requests.get(search_url, params=params, headers=headers, timeout=15)
                response.raise_for_status()
            except requests.exceptions.RequestException:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä»£ç†
                proxies = {
                    "http": "http://127.0.0.1:7890",
                    "https": "http://127.0.0.1:7890"
                }
                response = requests.get(search_url, params=params, headers=headers, timeout=15, proxies=proxies)
                response.raise_for_status()
            
            # è®¾ç½®æ­£ç¡®çš„ç¼–ç 
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°éªŒè¯ç 
            if "è¾“å…¥éªŒè¯ç " in response.text or "è¯·è¾“å…¥éªŒè¯ç " in response.text:
                return "âš ï¸ ç™¾åº¦æœç´¢é‡åˆ°éªŒè¯ç éªŒè¯ï¼Œè¯·ç¨åé‡è¯•"
            
            results = []
            
            # ç™¾åº¦æœç´¢ç»“æœçš„å¤šç§é€‰æ‹©å™¨
            # é€‰æ‹©å™¨1: æ ‡å‡†æœç´¢ç»“æœ
            search_results = soup.find_all('div', {'class': 'result'})
            
            # é€‰æ‹©å™¨2: æ–°ç‰ˆç™¾åº¦ç»“æœ
            if not search_results:
                search_results = soup.find_all('div', {'class': 'c-container'})
            
            # é€‰æ‹©å™¨3: é€šç”¨ç»“æœå®¹å™¨
            if not search_results:
                search_results = soup.find_all('div', {'tpl': True})
            
            # é€‰æ‹©å™¨4: æœ€é€šç”¨çš„é€‰æ‹©å™¨
            if not search_results:
                search_results = soup.find_all('div', recursive=True)
                search_results = [div for div in search_results 
                                if div.find('h3') and div.find('a') 
                                and len(div.get_text().strip()) > 50]
            
            for i, result in enumerate(search_results[:num_results]):
                try:
                    # æå–æ ‡é¢˜
                    title_elem = result.find('h3') or result.find('a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text().strip()
                    if not title:
                        continue
                    
                    # æå–é“¾æ¥
                    link_elem = result.find('a')
                    if not link_elem or not link_elem.get('href'):
                        continue
                    
                    link = link_elem.get('href')
                    
                    # æ¸…ç†ç™¾åº¦é‡å®šå‘é“¾æ¥
                    if link.startswith('http://www.baidu.com/link?url='):
                        # ç™¾åº¦çš„é‡å®šå‘é“¾æ¥ï¼Œæš‚æ—¶ä¿ç•™åŸé“¾æ¥
                        pass
                    elif link.startswith('/'):
                        # ç›¸å¯¹é“¾æ¥ï¼Œè·³è¿‡
                        continue
                    
                    # æå–æè¿°æ–‡æœ¬
                    snippet = "æ— æè¿°"
                    
                    # ç™¾åº¦æè¿°çš„å¤šç§é€‰æ‹©å™¨
                    snippet_selectors = [
                        '.c-abstract',
                        '.c-span9',
                        '.c-span-last',
                        'span[style*="color:#999"]',
                        '.c-color-text',
                        'span.c-color-gray',
                        'font[size="-1"]'
                    ]
                    
                    for selector in snippet_selectors:
                        snippet_elem = result.select_one(selector)
                        if snippet_elem:
                            snippet = snippet_elem.get_text().strip()
                            # æ¸…ç†ç™¾åº¦ç‰¹æœ‰çš„æ ‡è®°
                            snippet = snippet.replace('...', '').strip()
                            break
                    
                    # å¦‚æœä»ç„¶æ²¡æœ‰æè¿°ï¼Œå°è¯•ä»æ•´ä¸ªç»“æœä¸­æå–
                    if snippet == "æ— æè¿°":
                        text_content = result.get_text().strip()
                        if len(text_content) > len(title) + 20:
                            snippet = text_content.replace(title, '', 1).strip()
                            # å–å‰200ä¸ªå­—ç¬¦
                            snippet = snippet[:200] + ('...' if len(snippet) > 200 else '')
                    
                    # æ¸…ç†æ ‡é¢˜å’Œæè¿°ä¸­çš„å¤šä½™ç©ºç™½
                    title = ' '.join(title.split())
                    snippet = ' '.join(snippet.split())
                    
                    if title and link:
                        results.append({
                            'title': title,
                            'link': link,
                            'snippet': snippet
                        })
                        
                except Exception as e:
                    continue  # è·³è¿‡æœ‰é—®é¢˜çš„ç»“æœ
            
            if results:
                return json.dumps(results, ensure_ascii=False, indent=2)
            else:
                return "ğŸ” æœªæ‰¾åˆ°æœç´¢ç»“æœï¼Œå¯èƒ½çš„åŸå› ï¼š\n1. æœç´¢è¯è¿‡äºç‰¹æ®Š\n2. ç½‘ç»œè¿æ¥é—®é¢˜\n3. ç™¾åº¦é¡µé¢ç»“æ„å˜åŒ–\n\nå»ºè®®å°è¯•å…¶ä»–æœç´¢å·¥å…·ã€‚"
                
        except requests.exceptions.ProxyError:
            return "âŒ ä»£ç†è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç†è®¾ç½®"
        except requests.exceptions.ConnectionError:
            return "âŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        except requests.exceptions.Timeout:
            return "âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        except Exception as e:
            return f"âŒ ç™¾åº¦æœç´¢å¤±è´¥: {str(e)}\nå»ºè®®ä½¿ç”¨å…¶ä»–æœç´¢å·¥å…·"

    # âœ… è·å–æ‰€æœ‰å·¥å…·åˆ—è¡¨
    def get_tools(self) -> List:
        """
        è¿”å›æ‰€æœ‰å¯ç”¨å·¥å…·çš„åˆ—è¡¨ï¼Œä¾› Agent ä½¿ç”¨
        """
        return [
            self.search_tool,           # Tavily æœç´¢
            self.baidu_search,          # ç™¾åº¦æœç´¢
            self.web_scraping,          # ç½‘é¡µæŠ“å–
            self.datetime_operations,   # æ—¥æœŸæ—¶é—´æ“ä½œ
            self.get_realtime_weather,  # å®æ—¶å¤©æ°”æŸ¥è¯¢
        ]

    # âœ… æ‰“å°æ‰€æœ‰å·¥å…·åç§°ï¼ˆè°ƒè¯•ç”¨ï¼‰
    def print_tools(self):
        """æ‰“å°æ‰€æœ‰å·¥å…·åç§°å’Œæè¿°"""
        tools = self.get_tools()
        for tool in tools:
            print(f"ğŸ”§ {tool.name}: {tool.description}")


if __name__ == "__main__":
    # ================== åˆ›å»º AgentExecutor ==================
    from langchain_openai import ChatOpenAI
    # from langchain import hub
    from langchain.agents import create_tool_calling_agent, AgentExecutor
    load_dotenv("è¯¾ç¨‹åŠ©æ‰‹/lna.env")
    # ğŸ”§ 1. åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼ˆä»¥ GPT-3.5-turbo ä¸ºä¾‹ï¼‰
    llm = ChatOpenAI(
        model="qwen-max",
        # temperature=0,
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        streaming=True
    )

    tool_ = ToolManager()
    # ğŸ”§ 2. å‡†å¤‡å·¥å…·åˆ—è¡¨
    # tools = [
    #     search_tool,      # Tavily æœç´¢
    #     web_scraping,      # ç½‘é¡µæŠ“å–
    #     get_realtime_weather, # å®æ—¶å¤©æ°”æŸ¥è¯¢
    #     datetime_operations # æ—¥æœŸæ—¶é—´æ“ä½œ
    # ]


    # ğŸ”§ 3. åˆ›å»º promptï¼ˆä½¿ç”¨ LangChain å®˜æ–¹æ¨èçš„ agent promptï¼‰
    # ä½ å¯ä»¥è‡ªå®šä¹‰ï¼Œä¹Ÿå¯ä»¥ç”¨ hub ä¸Šçš„æ ‡å‡†æ¨¡æ¿
    # âœ… åˆ›å»ºæç¤ºè¯æ¨¡æ¿
    system_prompt = """
    ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿å¸®åŠ©ç”¨æˆ·é«˜æ•ˆå®Œæˆå„ç§ä»»åŠ¡ã€‚ä½ æ‹¥æœ‰ä»¥ä¸‹å¼ºå¤§çš„å·¥å…·èƒ½åŠ›ï¼š

    ## ğŸ” æœç´¢å’Œä¿¡æ¯è·å–å·¥å…·
    1. **Tavilyæœç´¢ (search_tool)**: ğŸ¥‡ ä¸“ä¸šæœç´¢APIï¼Œç¨³å®šå¯é ï¼Œé€‚åˆè·å–æœ€æ–°ä¿¡æ¯å’Œæ–°é—»
    2. **ç™¾åº¦æœç´¢ (baidu_search)**: ğŸŒŸ å¯¹ä¸­æ–‡æœç´¢å‹å¥½ï¼Œæ— åçˆ¬è™«é—®é¢˜ï¼Œæ¨èç”¨äºä¸­æ–‡å†…å®¹æœç´¢
    3. **ç½‘é¡µæŠ“å– (web_scraping)**: ğŸ“„ æŠ“å–æŒ‡å®šç½‘é¡µçš„å†…å®¹ï¼Œå¯è·å–æ–‡æœ¬æˆ–HTMLæºç 
    4. **å®æ—¶å¤©æ°”æŸ¥è¯¢ (get_realtime_weather)**: â˜€ï¸ æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å®æ—¶å¤©æ°”æƒ…å†µ
    5. **æ—¥æœŸæ—¶é—´æ“ä½œ (datetime_operations)**: ğŸ• è·å–å½“å‰æ—¶é—´ã€æ ¼å¼åŒ–æ—¥æœŸã€æ—¥æœŸè®¡ç®—ç­‰

    ## ğŸ¯ å·¥å…·ä½¿ç”¨ç­–ç•¥

    **æœç´¢ä¿¡æ¯æ—¶ï¼š**
    - ğŸ¥‡ é¦–é€‰ï¼š`search_tool` (Tavily) - ä¸“ä¸šã€ç¨³å®šã€å‡†ç¡®
    - ğŸŒŸ ä¸­æ–‡æœç´¢ï¼š`baidu_search` - ä¸­æ–‡å†…å®¹å‹å¥½ï¼Œç¨³å®šå¯é 
    - ğŸ“„ ç‰¹å®šé¡µé¢ï¼š`web_scraping` - è·å–å…·ä½“ç½‘é¡µå†…å®¹

    **å…¶ä»–å·¥å…·ï¼š**
    - â˜€ï¸ å¤©æ°”æŸ¥è¯¢ï¼š`get_realtime_weather`
    - ğŸ• æ—¶é—´å¤„ç†ï¼š`datetime_operations`

    **æœç´¢é¡ºåºå»ºè®®ï¼š** Tavily â†’ ç™¾åº¦æœç´¢
    """

    from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        # MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad")
    ])

    # ğŸ”§ 4. åˆ›å»º agent
    agent = create_tool_calling_agent(
        llm=llm,
        tools=tool_.get_tools(),
        prompt=prompt
    )

    # ğŸ”§ 5. åˆ›å»º AgentExecutor
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tool_.get_tools(),
        verbose=True,           # æ‰“å°æ€è€ƒè¿‡ç¨‹
        handle_parsing_errors=True  # è‡ªåŠ¨å¤„ç†è§£æé”™è¯¯
    )

    # ================== æµ‹è¯• Agent ==================
    print("ğŸ” æ­£åœ¨æµ‹è¯• Agentï¼Œè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰ï¼š\n")
    while True:
        query = input("User: ").strip()
        if query.lower() in ["quit", "exit", "é€€å‡º"]:
            print("ğŸ‘‹ å†è§ï¼")
            break
        if query:
            try:
                result = agent_executor.invoke({"input": query})
                print(f"AI: {result['output']}\n")
            except Exception as e:
                print(f"âŒ æ‰§è¡Œå‡ºé”™: {e}\n")